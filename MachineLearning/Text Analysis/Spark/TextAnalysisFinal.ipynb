{"cells":[{"cell_type":"markdown","source":["## Text Analysis\nTo create a classification model that analyse tip text to predict the likes.\n### Import Spark SQL and Spark ML Libraries\n\nFirst, import the libraries you will need:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Load Source Data\nNow load the tweets data into a DataFrame. This data consists of tweets that have been previously captured and classified as positive or negative."],"metadata":{}},{"cell_type":"code","source":["text_csv = sqlContext.sql(\"Select * from tipcleaned\")\n\ntext_csv.show(5)"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(text_csv.groupBy(\"likes\").count().orderBy(\"likes\"))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Prepare the Data\nThe features for the classification model will be derived from the tip text. The label is the like (between 1-10)"],"metadata":{}},{"cell_type":"code","source":["textdata = text_csv.select(\"text\", col(\"likes\").cast(\"Int\").alias(\"label\"))\ntextdata.show(truncate = False)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Split the Data\nIn common with most classification modeling processes, you'll split the data into a set for training, and a set for testing the trained model."],"metadata":{}},{"cell_type":"code","source":["splits = textdata.randomSplit([0.7, 0.3],seed=0)\ntextrain = splits[0]\ntextest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\ntextrain_rows = textrain.count()\ntextest_rows = textest.count()\nprint \"Training Rows:\", textrain_rows, \" Testing Rows:\", textest_rows"],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"code","source":["textest.show(5)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["textdata.show(5,truncate = False)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Define the Pipeline\nThe pipeline for the model consist of the following stages:\n- A Tokenizer to split the tweets into individual words.\n- A StopWordsRemover to remove common words such as \"a\" or \"the\" that have little predictive value.\n- A HashingTF class to generate numeric vectors from the text values.\n- A LogisticRegression algorithm to train a binary classification model."],"metadata":{}},{"cell_type":"code","source":["# convert sentence to words' list\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"Words\")\n# remove stop words\nswr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"MeaningfulWords\")\n# convert word to number as word frequency\nhashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n# set the model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n\n# process pipeline with the series of transforms - 4 transforms\npipeline = Pipeline(stages=[tokenizer, swr, hashTF, lr])"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Run the Pipeline as an Estimator\nThe pipeline itself is an estimator, and so it has a **fit** method that you can call to run the pipeline on a specified DataFrame. In this case, you will run the pipeline on the training data to train a model."],"metadata":{}},{"cell_type":"code","source":["piplineModel = pipeline.fit(textrain)\nprint \"Pipeline complete!\""],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Test the Pipeline Model\nThe model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, you will transform the **test** DataFrame using the pipeline to generate label predictions."],"metadata":{}},{"cell_type":"code","source":["prediction = piplineModel.transform(textest)\npredicted = prediction.select(\"text\", \"prediction\", \"trueLabel\")\npredicted.show(10)"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":17},{"cell_type":"code","source":["predicted10 = prediction.select(\"*\")\npredicted10.show(10)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted10.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted10.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted10.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted10.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n# LogisticRegression: rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\"\nevaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(prediction)\nprint \"AUR = \", aur\n"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"kernelspec":{"display_name":"Python 2 with Spark 2.0","language":"python","name":"python2-spark20"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.11","nbconvert_exporter":"python","file_extension":".py"},"name":"TextAnalysisFinal","notebookId":1181611669862246},"nbformat":4,"nbformat_minor":0}
