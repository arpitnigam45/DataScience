{"cells":[{"cell_type":"markdown","source":["### Classification\n\n### Prediction for FOOD related business\nTo have clearity in understanding which feature columns contributes in our prediction we can broadly categorize the business into different Categories like Food, Entertaiment, Medical, Services, Shooping, Education etc. For predicting the popularity of the Yelp business we decide to choose Food related business and feature columns.\n\nSub categories under Food category are 'Wine Bars','Vietnamese','vegetarian','vegan','Turkish','Thai','Tex-Mex','Tea Rooms','Tapas/Small Plates','Tapas Bars','Taiwanese','Szechuan','Sushi Bars','Steakhouses','Soup','Soul Food','Seafood','Sandwiches','Salad','Russian','Restaurants','restaurant' etc.\n\nThe feature columns related to food are review_count,stars,Take-out,GoodFor_lunch,GoodFor_dinner,GoodFor_breakfast,Noise_Level, Takes_Reservations,Delivery,Parking_lot,WheelchairAccessible,Alcohol,WaiterService,Wi-Fi."],"metadata":{}},{"cell_type":"markdown","source":["<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/></a>\n<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n\n#### Author: [Ruchi Singh](https://www.linkedin.com/in/ruchi-singh-68015945/)\n\n#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n\n#### Date: 05/20/2017"],"metadata":{}},{"cell_type":"markdown","source":["## Download Data\n\ndownload the \"Business-Food.csv\" file and upload in Databricks. Data-> default-> Create Table. Rename the table as \"Food2\" and check for all the columns datatype. \n\nThis is the data to be used for training the machine learning algorithm."],"metadata":{}},{"cell_type":"markdown","source":["## Logestic regression\n\nThe Logestic Regression classification model is used to predict the stars (popularity) for the business.The assumtion made here is that the business is unpopular if the Star is less than 3 and the business is popular if the Stars are more than 3.\n\n### Prepare the Data\nFirst, import the libraries you will need and prepare the training and test data:"],"metadata":{}},{"cell_type":"code","source":["# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Load Food table\nFood table created is now loaded in Spark using SQL query:"],"metadata":{}},{"cell_type":"code","source":["# Load the source data\ncsv = sqlContext.sql(\"Select * from food2\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Select features and label\n# Logistic Regression\ndata = csv.select(\"review_count\",\"Take-out\", \"GoodFor_lunch\", \"GoodFor_dinner\", \"GoodFor_breakfast\",\"Noise_Level\", \"Takes_Reservations\",\"Delivery\",\"Parking_lot\", \"WheelchairAccessible\",\"Alcohol\", \"WaiterService\",\"Wi-Fi\",\"stars\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":["data.show(5)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### String Indexer\nStringIndexer encodes a string column of labels to a column of label indices."],"metadata":{}},{"cell_type":"code","source":["def indexStringColumns(df, cols):\n    #variable newdf will be updated several times\n    newdata = df\n    for c in cols:\n        si = StringIndexer(inputCol=c, outputCol=c+\"-x\")\n        sm = si.fit(newdata)\n        newdata = sm.transform(newdata).drop(c)\n        newdata = newdata.withColumnRenamed(c+\"-x\", c)\n    return newdata\n\ndfnumeric = indexStringColumns(data, [\"Take-out\",\"GoodFor_lunch\", \"GoodFor_dinner\", \"GoodFor_breakfast\",\"Noise_Level\", \"Takes_Reservations\",\"Delivery\",\"Parking_lot\", \"WheelchairAccessible\",\"Alcohol\", \"WaiterService\",\"Wi-Fi\"])\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["dfnumeric.show(25)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Encoder\nOne-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, in classification model, to use categorical features."],"metadata":{}},{"cell_type":"code","source":["def oneHotEncodeColumns(df, cols):\n    from pyspark.ml.feature import OneHotEncoder\n    newdf = df\n    for c in cols:\n        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n        newdf = onehotenc.transform(newdf).drop(c)\n        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n    return newdf\n\ndfhot = oneHotEncodeColumns(dfnumeric, [\"Take-out\",\"GoodFor_lunch\", \"GoodFor_dinner\", \"GoodFor_breakfast\",\"Noise_Level\", \"Takes_Reservations\",\"Delivery\",\"Parking_lot\", \"WheelchairAccessible\",\"Alcohol\", \"WaiterService\",\"Wi-Fi\"])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["dfhot.show(25)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Vector Assembler\nVectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression."],"metadata":{}},{"cell_type":"code","source":["va = VectorAssembler(outputCol=\"features\", inputCols=list(set(dfhot.columns)-set(['stars'])))\nlpoints = va.transform(dfhot).select(\"features\", \"stars\").withColumnRenamed(\"stars\",\"label\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Data Split\nSplit the data into training and test data in the ratio 80:20 using a random split."],"metadata":{}},{"cell_type":"code","source":["# Split the data\nsplits = lpoints.randomSplit([0.8, 0.2])\nadulttrain = splits[0].cache()\nadultvalid = splits[1].cache()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a classification model"],"metadata":{}},{"cell_type":"code","source":["lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\nlrmodel = lr.fit(adulttrain)\nlrmodel = lr.setParams(regParam=0.01, maxIter=500, fitIntercept=True).fit(adulttrain)\nlrmodel.intercept\n\nvalidpredicts = lrmodel.transform(adultvalid)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["validpredicts.show(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Evaluate the model\nUsing a BinaryClassificationEvaluator the classification model used on the data is evaluated."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nbceval = BinaryClassificationEvaluator()\nbceval.evaluate(validpredicts)\nbceval.getMetricName()\n\nbceval.setMetricName(\"areaUnderPR\")\nbceval.evaluate(validpredicts)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(validpredicts)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Cross validation\nIt is is to ensure that every example from the original dataset has the same chance of appearing in the training and testing set."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator\ncv = CrossValidator().setEstimator(lr).setEvaluator(bceval).setNumFolds(2)\nparamGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000]).addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()\ncv.setEstimatorParamMaps(paramGrid)\ncvmodel = cv.fit(adulttrain)\n\nBinaryClassificationEvaluator().evaluate(cvmodel.bestModel.transform(adultvalid))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. A simple way to do this is to use  **TrainValidationSplit** to evaluate each combination of parameters defined in a **ParameterGrid** against a subset of the training data in order to find the best performing parameters.\n\n#### Regularization \nIt is a way of avoiding Imbalances in the way that the data is trained against the training data so that the model ends up being over fit to the training data. In other words It works really well with the training data but it doesn't generalize well with other data.\nThat we can use a **regularization parameter** to vary the way that the model balances that way.\n\n#### Training ratio of 0.8\nIt is going to use 80% of the the data that it's got in its training set to train the model and then the remaining 20% is going to use to validate the trained model. \n\nIn **ParamGridBuilder**, all possible combinations are generated from regParam, maxIter, threshold. So it is going to try each combination of the parameters with 80% of the the data to train the model and 20% to to validate it."],"metadata":{}},{"cell_type":"code","source":["# LogisticRegression with attribute 'threshold' in ParamGridBuilder and BinaryClassificationEvaluator\nparamGrid = ParamGridBuilder().addGrid(lr.regParam, [0.3, 0.1, 0.01]).addGrid(lr.maxIter, [10, 5]).addGrid(lr.threshold, [0.35, 0.30]).build()\n\ntvs = TrainValidationSplit(estimator=lr, evaluator=RegressionEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\nmodel = tvs.fit(adulttrain)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["prediction = model.transform(adultvalid)\n# LogisticRegression\npredicted = prediction.select(\"features\", \"prediction\", \"probability\", \"label\")\n\npredicted.show(100)"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Compute Confusion Matrix Metrics: Only for Classification Logistic Regression not for Linear Regression\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated.\n\n### Result\nPrecision (0.8762570727816253), Recall (0.7303376371612134): Precision becomes a little bit lower but the precision becomes much higher than previous no tuning example."],"metadata":{}},{"cell_type":"code","source":["# Only for Classification Logistic Regression \n\ntp = float(predicted.filter(\"prediction == 1.0 AND label == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND label == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND label == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND label == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Review the Area Under ROC: Only for Classification Logistic Regression \nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this."],"metadata":{}},{"cell_type":"code","source":["display(metrics)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(validpredicts)\nprint \"AUR = \", aur"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":37}],"metadata":{"kernelspec":{"display_name":"Python 2 with Spark 2.0","language":"python","name":"python2-spark20"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.11","nbconvert_exporter":"python","file_extension":".py"},"name":"Classification-Fooddata","notebookId":1574170486057485},"nbformat":4,"nbformat_minor":0}
